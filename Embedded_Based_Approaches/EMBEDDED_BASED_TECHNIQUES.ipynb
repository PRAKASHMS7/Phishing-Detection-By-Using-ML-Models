{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJasJOOKT5baLfxu3unRHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRAKASHMS7/Phishing-Detection-By-Using-ML-Models/blob/main/Embedded_Based_Approaches/EMBEDDED_BASED_TECHNIQUES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGULARIZATION TECHNIQUE"
      ],
      "metadata": {
        "id": "1vTUCdTbUTjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/All.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('URL_Type_obf_Type', axis=1)  # Features\n",
        "y = data['URL_Type_obf_Type']  # Target\n",
        "\n",
        "# Convert categorical target to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Handle missing and infinite values\n",
        "X.replace([float('inf'), float('-inf')], np.nan, inplace=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply L1 Regularization (Lasso) for feature selection\n",
        "logreg = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "logreg.fit(X_scaled, y_encoded)\n",
        "\n",
        "# Get absolute feature importance\n",
        "importance = np.abs(logreg.coef_).sum(axis=0)\n",
        "\n",
        "# Select the 40 most important features **while maintaining the original order**\n",
        "top_40_indices = np.argsort(importance)[-40:]  # Get indices of top 40 features\n",
        "selected_features = X.columns[top_40_indices]  # Feature names\n",
        "\n",
        "# Keep only the selected features in the same order as they appear in X\n",
        "X_selected = X[selected_features]  # Extract from original dataframe (preserves order)\n",
        "\n",
        "# Rename columns with serial numbers (1, 2, 3, ...)\n",
        "X_selected.columns = [f\"{i+1}. {col}\" for i, col in enumerate(X_selected.columns)]\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression classifier on the selected features\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "auc_score = roc_auc_score(y_test, y_prob, multi_class='ovr', average='weighted')\n",
        "\n",
        "# Generate classification report\n",
        "class_report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "# Print results\n",
        "print(\"Selected Features in Serial Order:\")\n",
        "for i, feature in enumerate(selected_features, 1):\n",
        "    print(f\"{i}. {feature}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMaY7KqiaFwL",
        "outputId": "cddb97bc-ac4f-4485-8465-738c3b66bbba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features in Serial Order:\n",
            "1. Directory_LetterCount\n",
            "2. NumberRate_Domain\n",
            "3. delimeter_path\n",
            "4. tld\n",
            "5. dld_domain\n",
            "6. domain_token_count\n",
            "7. Entropy_Afterpath\n",
            "8. host_letter_count\n",
            "9. SymbolCount_URL\n",
            "10. Extension_DigitCount\n",
            "11. host_DigitCount\n",
            "12. avgdomaintokenlen\n",
            "13. SymbolCount_Afterpath\n",
            "14. charcompvowels\n",
            "15. domainlength\n",
            "16. URLQueries_variable\n",
            "17. argPathRatio\n",
            "18. ldl_url\n",
            "19. path_token_count\n",
            "20. delimeter_Count\n",
            "21. Extension_LetterCount\n",
            "22. longdomaintokenlen\n",
            "23. URL_DigitCount\n",
            "24. ArgUrlRatio\n",
            "25. SymbolCount_Domain\n",
            "26. ldl_getArg\n",
            "27. pathDomainRatio\n",
            "28. argDomanRatio\n",
            "29. Query_DigitCount\n",
            "30. subDirLen\n",
            "31. pathLength\n",
            "32. domainUrlRatio\n",
            "33. LongestVariableValue\n",
            "34. pathurlRatio\n",
            "35. Query_LetterCount\n",
            "36. LongestPathTokenLength\n",
            "37. urlLen\n",
            "38. ArgLen\n",
            "39. Querylength\n",
            "40. URL_Letter_Count\n",
            "\n",
            "Accuracy: 0.7834\n",
            "Precision: 0.7841\n",
            "Recall: 0.7834\n",
            "F1 Score: 0.7795\n",
            "AUC Score: 0.9503\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Defacement       0.80      0.79      0.79      1628\n",
            "      benign       0.77      0.89      0.83      1526\n",
            "     malware       0.75      0.55      0.63      1332\n",
            "    phishing       0.72      0.80      0.76      1497\n",
            "        spam       0.89      0.87      0.88      1359\n",
            "\n",
            "    accuracy                           0.78      7342\n",
            "   macro avg       0.79      0.78      0.78      7342\n",
            "weighted avg       0.78      0.78      0.78      7342\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FOREST**"
      ],
      "metadata": {
        "id": "b5QGl5whbIdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/All.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('URL_Type_obf_Type', axis=1)  # Features\n",
        "y = data['URL_Type_obf_Type']  # Target\n",
        "\n",
        "# Convert target variable to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Handle missing and infinite values\n",
        "X.replace([float('inf'), float('-inf')], np.nan, inplace=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "# Extract the 40 most important features\n",
        "important_features = importances.sort_values(ascending=False).head(40)\n",
        "\n",
        "# Keep features in the order they appear in the dataset\n",
        "selected_features = X.columns[X.columns.isin(important_features.index)]\n",
        "\n",
        "# Number the selected features\n",
        "numbered_features = [f\"{i+1}. {feature}\" for i, feature in enumerate(selected_features)]\n",
        "\n",
        "# Select only the important features\n",
        "X_selected_train = X_train[:, [list(X.columns).index(f) for f in selected_features]]\n",
        "X_selected_test = X_test[:, [list(X.columns).index(f) for f in selected_features]]\n",
        "\n",
        "# Train a new Random Forest model with selected features\n",
        "rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_selected.fit(X_selected_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_selected.predict(X_selected_test)\n",
        "y_prob = rf_selected.predict_proba(X_selected_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "auc_score = roc_auc_score(y_test, y_prob, multi_class='ovr', average='weighted')\n",
        "\n",
        "# Print the 40 most important features\n",
        "print(\"Top 40 Important Features (with Serial Numbers):\")\n",
        "for feature in numbered_features:\n",
        "    print(feature)\n",
        "\n",
        "# Print performance metrics\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8WkncxRbMgZ",
        "outputId": "02d4232c-0d40-4424-87cd-0045c194166e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 40 Important Features (with Serial Numbers):\n",
            "1. domain_token_count\n",
            "2. avgdomaintokenlen\n",
            "3. longdomaintokenlen\n",
            "4. avgpathtokenlen\n",
            "5. tld\n",
            "6. ldl_path\n",
            "7. urlLen\n",
            "8. domainlength\n",
            "9. pathLength\n",
            "10. subDirLen\n",
            "11. pathurlRatio\n",
            "12. ArgUrlRatio\n",
            "13. argDomanRatio\n",
            "14. domainUrlRatio\n",
            "15. pathDomainRatio\n",
            "16. argPathRatio\n",
            "17. NumberofDotsinURL\n",
            "18. CharacterContinuityRate\n",
            "19. LongestVariableValue\n",
            "20. URL_DigitCount\n",
            "21. Extension_DigitCount\n",
            "22. URL_Letter_Count\n",
            "23. host_letter_count\n",
            "24. Directory_LetterCount\n",
            "25. Filename_LetterCount\n",
            "26. Extension_LetterCount\n",
            "27. LongestPathTokenLength\n",
            "28. Domain_LongestWordLength\n",
            "29. Arguments_LongestWordLength\n",
            "30. spcharUrl\n",
            "31. delimeter_path\n",
            "32. NumberRate_URL\n",
            "33. NumberRate_FileName\n",
            "34. SymbolCount_URL\n",
            "35. SymbolCount_Domain\n",
            "36. SymbolCount_Directoryname\n",
            "37. SymbolCount_FileName\n",
            "38. SymbolCount_Extension\n",
            "39. Entropy_Domain\n",
            "40. Entropy_DirectoryName\n",
            "\n",
            "Accuracy: 0.9773\n",
            "Precision: 0.9775\n",
            "Recall: 0.9773\n",
            "F1 Score: 0.9773\n",
            "AUC Score: 0.9990\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Defacement       0.98      0.98      0.98      1628\n",
            "      benign       0.97      0.99      0.98      1526\n",
            "     malware       0.99      0.98      0.99      1332\n",
            "    phishing       0.94      0.96      0.95      1497\n",
            "        spam       0.99      0.98      0.99      1359\n",
            "\n",
            "    accuracy                           0.98      7342\n",
            "   macro avg       0.98      0.98      0.98      7342\n",
            "weighted avg       0.98      0.98      0.98      7342\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold, SelectKBest, SelectFromModel\n",
        "from time import time\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading data...\")\n",
        "data = pd.read_csv('/content/All.csv')\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = data.drop(columns=['URL_Type_obf_Type'])\n",
        "y = data['URL_Type_obf_Type']\n",
        "\n",
        "# 1. Handle infinite and missing values more efficiently\n",
        "print(\"Handling missing values...\")\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# 2. Clip extreme values - fixed implementation\n",
        "print(\"Clipping extreme values...\")\n",
        "lower = X.quantile(0.01)\n",
        "upper = X.quantile(0.99)\n",
        "# Clip each column separately\n",
        "X = X.apply(lambda col: np.clip(col, lower[col.name], upper[col.name]), axis=0)\n",
        "\n",
        "# 3. Remove low variance features\n",
        "print(\"Removing low variance features...\")\n",
        "var_thresh = VarianceThreshold(threshold=0.01)\n",
        "X = var_thresh.fit_transform(X)\n",
        "\n",
        "# 4. Select top 60 features using Mutual Information\n",
        "print(\"Selecting top features with Mutual Information...\")\n",
        "start_time = time()\n",
        "mi_selector = SelectKBest(mutual_info_classif, k=min(60, X.shape[1]))  # Ensure k <= number of features\n",
        "X = mi_selector.fit_transform(X, y)\n",
        "print(f\"Feature selection took {time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Split the data\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "print(\"Scaling features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Logistic Regression Model with optimized parameters\n",
        "print(\"Initializing model...\")\n",
        "model = LogisticRegression(random_state=42, max_iter=300, solver='lbfgs', n_jobs=-1)\n",
        "\n",
        "# Feature selection with L1 regularization\n",
        "print(\"Performing feature selection with L1 regularization...\")\n",
        "sfm = SelectFromModel(\n",
        "    LogisticRegression(penalty='l1', solver='liblinear', random_state=42, max_iter=100),\n",
        "    max_features=40  # Select top 40 features\n",
        ")\n",
        "sfm.fit(X_train, y_train)\n",
        "X_train_selected = sfm.transform(X_train)\n",
        "X_test_selected = sfm.transform(X_test)\n",
        "\n",
        "# Get selected feature indices (relative to the 60 features)\n",
        "selected_features = sfm.get_support(indices=True)\n",
        "print(f\"\\nSelected {len(selected_features)} features\")\n",
        "\n",
        "# Train final model\n",
        "print(\"Training final model...\")\n",
        "model.fit(X_train_selected, y_train)\n",
        "y_pred = model.predict(X_test_selected)\n",
        "y_probs = model.predict_proba(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "roc_auc = roc_auc_score(y_test, y_probs, multi_class='ovr', average='macro')\n",
        "\n",
        "# Display performance results\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
        "print(f\"ROC AUC: {roc_auc * 100:.2f}%\\n\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoJbgOx2BdfD",
        "outputId": "66287f3f-e521-4cf3-cd4b-c62778efc863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Handling missing values...\n",
            "Clipping extreme values...\n",
            "Removing low variance features...\n",
            "Selecting top features with Mutual Information...\n",
            "Feature selection took 22.37 seconds\n",
            "Splitting data...\n",
            "Scaling features...\n",
            "Initializing model...\n",
            "Performing feature selection with L1 regularization...\n",
            "\n",
            "Selected 40 features\n",
            "Training final model...\n",
            "\n",
            "Performance Metrics:\n",
            "Accuracy: 80.48%\n",
            "Precision: 80.43%\n",
            "Recall: 80.13%\n",
            "F1 Score: 80.05%\n",
            "ROC AUC: 95.89%\n",
            "\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Defacement       0.84      0.83      0.83      1628\n",
            "      benign       0.79      0.90      0.84      1526\n",
            "     malware       0.77      0.62      0.69      1332\n",
            "    phishing       0.75      0.79      0.77      1497\n",
            "        spam       0.87      0.87      0.87      1359\n",
            "\n",
            "    accuracy                           0.80      7342\n",
            "   macro avg       0.80      0.80      0.80      7342\n",
            "weighted avg       0.80      0.80      0.80      7342\n",
            "\n"
          ]
        }
      ]
    }
  ]
}