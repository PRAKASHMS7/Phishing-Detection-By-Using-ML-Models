{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRAKASHMS7/Phishing-Detection-By-Using-ML-Models/blob/main/Model/Model_Training_FULLY_CLEANED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZN3hqr0ceO0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRAKASHMS7/Phishing-Detection-By-Using-ML-Models/blob/main/Model_Training/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c6LSHuGceO2"
      },
      "outputs": [],
      "source": [
        "pip install pycaret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME79eaGrceO4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pycaret.classification import *\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "\n",
        "# Step 2: Load Your Dataset\n",
        "data = pd.read_csv('/content/All.csv')  # <-- Adjust path if needed\n",
        "\n",
        "# Step 3: Select Relevant Features\n",
        "selected_features = [\n",
        "    'NumberofDotsinURL', 'avgdomaintokenlen', 'avgpathtokenlen', 'tld', 'urlLen', 'domainlength',\n",
        "    'pathLength', 'subDirLen', 'pathurlRatio', 'ArgUrlRatio', 'argDomanRatio', 'domainUrlRatio',\n",
        "    'pathDomainRatio', 'argPathRatio', 'CharacterContinuityRate', 'LongestVariableValue',\n",
        "    'URL_DigitCount', 'Extension_DigitCount', 'URL_Letter_Count', 'host_letter_count', 'Directory_LetterCount',\n",
        "    'Extension_LetterCount', 'LongestPathTokenLength', 'Domain_LongestWordLength', 'Arguments_LongestWordLength',\n",
        "    'spcharUrl', 'delimeter_path', 'NumberRate_URL', 'NumberRate_FileName', 'SymbolCount_URL',\n",
        "    'SymbolCount_Domain', 'SymbolCount_Directoryname', 'SymbolCount_FileName', 'SymbolCount_Extension',\n",
        "    'Entropy_Domain', 'Entropy_DirectoryName', 'domain_token_count'\n",
        "]\n",
        "\n",
        "# Filter the data to include only the selected features and the target\n",
        "data = data[selected_features + ['URL_Type_obf_Type']]  # Assuming 'URL_Type_obf_Type' is your target\n",
        "\n",
        "# Step 4: Check and Clean the Data\n",
        "numeric_data = data.select_dtypes(include=[np.number])\n",
        "\n",
        "# Check for NaN and infinite values in numeric columns\n",
        "if np.any(np.isnan(numeric_data)) or np.any(np.isinf(numeric_data)):\n",
        "    print(\"Data contains NaN or infinite values. Cleaning data...\")\n",
        "\n",
        "    # Replace infinite values with NaN and then fill NaN with the median\n",
        "    numeric_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    data[numeric_data.columns] = numeric_data.fillna(numeric_data.median())  # Replace NaN with the median value\n",
        "\n",
        "# Step 5: Setup PyCaret AutoML\n",
        "clf = setup(\n",
        "    data=data,\n",
        "    target='URL_Type_obf_Type',  # Make sure 'URL_Type_obf_Type' is your target column name\n",
        "    session_id=123,\n",
        "    normalize=True,\n",
        "    use_gpu=False,\n",
        "    ignore_features=[],  # Add any irrelevant features to this list if needed\n",
        ")\n",
        "\n",
        "# Step 6: Compare All Models Automatically\n",
        "best_models = compare_models(\n",
        "    sort='Accuracy',     # Sort models by highest Accuracy\n",
        "    n_select=5           # Select Top 5 Models\n",
        ")\n",
        "\n",
        "# Step 7: Get the Results Table\n",
        "comparison_df = pull()\n",
        "\n",
        "# Step 8: Compute Additional Metrics Manually\n",
        "# Function to calculate TPR, TNR, FPR, FNR\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tp = cm[1, 1]  # True Positive\n",
        "    tn = cm[0, 0]  # True Negative\n",
        "    fp = cm[0, 1]  # False Positive\n",
        "    fn = cm[1, 0]  # False Negative\n",
        "\n",
        "    # Calculating metrics\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "    tpr = recall  # Same as Sensitivity\n",
        "    tnr = tn / (tn + fp) if (tn + fp) != 0 else 0  # Specificity\n",
        "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0    # False Positive Rate\n",
        "    fnr = fn / (fn + tp) if (fn + tp) != 0 else 0    # False Negative Rate\n",
        "    roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc, tpr, tnr, fpr, fnr\n",
        "\n",
        "# Step 9: Store the Model Metrics\n",
        "metrics_list = []\n",
        "for model in best_models:\n",
        "    print(f\"\\nðŸ”µ Evaluating Model: {model}\")\n",
        "    model_predictions = predict_model(model)\n",
        "\n",
        "    # Check column names of model_predictions to figure out the correct prediction column\n",
        "    print(\"Predicted Dataframe Columns:\", model_predictions.columns)\n",
        "\n",
        "    # Find the correct column name for predictions\n",
        "    predicted_column_name = model_predictions.columns[0]  # Usually the first column is the prediction\n",
        "\n",
        "    y_true = data['URL_Type_obf_Type'].values  # Actual labels\n",
        "    y_pred = model_predictions[predicted_column_name].values  # Use the correct prediction column\n",
        "\n",
        "    # Check if the lengths match\n",
        "    if len(y_true) == len(y_pred):\n",
        "        # Get the metrics for this model\n",
        "        metrics = calculate_metrics(y_true, y_pred)\n",
        "        metrics_list.append([model, *metrics])\n",
        "    else:\n",
        "        print(f\"Length mismatch for {model}. Skipping this model.\")\n",
        "\n",
        "# Step 10: Create a DataFrame for the Metrics\n",
        "metrics_df = pd.DataFrame(metrics_list, columns=[\n",
        "    'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC', 'TPR', 'TNR', 'FPR', 'FNR'\n",
        "])\n",
        "\n",
        "# Step 11: Print the Comparison Table\n",
        "print(\"\\nðŸ”µ Model Comparison Table ðŸ”µ\\n\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Step 12: (Optional) Save Table to CSV File\n",
        "# metrics_df.to_csv('model_comparison_results.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}